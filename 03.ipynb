{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Existing tools, automation, and connecting the dots\n",
    "\n",
    "We saw in Parts I and II that investigating websites in order to understand where ressources come from, and where your browser will get you is not trivial. And we did that on relatively simple websites. \n",
    "\n",
    "In this sections, we will look at existing tools and standards, what they are useful for, and their limitations.\n",
    "\n",
    "## Scraping / crawler\n",
    "\n",
    "A scraper will extract content from a page in a more or less automated way. All the search engines are doing that, you can build your own scraper to keep track of the availability of your prefered gaming console, check the price of a graphic card, or just copy and compare the content of a website on a regular basis.\n",
    "\n",
    "The old school one I used in the past is [httrack](https://github.com/xroche/httrack), there are fancier ones like [portia](https://github.com/scrapinghub/portia) for example. And if you want to write your own, [scrapy](https://github.com/scrapy/scrapy) is the way to go.\n",
    "\n",
    "Or just curl for the quick and dirty approach.\n",
    "\n",
    "**Problem**: You only get the rendered page and lose all the intermediary steps (or they're just hard to get to).\n",
    "\n",
    "## Inspect / network in the browser\n",
    "\n",
    "As it is a debug/dev tool, you can see everything. \n",
    "\n",
    "Play time! Open the inspect dialog in a few website and look at what's happening.\n",
    "\n",
    "**Problem**: Hard to replicate and compare a capture, especially on a website you're not very well informed about.\n",
    "\n",
    "**Taks**: Get a HAR file from the network tab, and look at the content.\n",
    "\n",
    "## HTTP Archive (HAR)\n",
    "\n",
    "It took us a long time, but it is now time to look at... a spec! The HTTP Archive (HAR) spec is available [here](http://www.softwareishard.com/blog/har-12-spec/). I promise you, it's way more interesting than ISO 27xxx. And it's not the HTTP RFCs.\n",
    "\n",
    "To make it simple, it is a list of every request and response of everything that happened in the process of rendering the page you see in your browser, and with all the headers in each direction.\n",
    "\n",
    "**Task**: Look at the HAR of a simple website and try to understand what is happening, which call is responsible for which other call, the headers, ... \n",
    "\n",
    "## Splash\n",
    "\n",
    "Splash is relatively close to the inspector in the browser, but standalone, with more configuration options, and an API.\n",
    "\n",
    "Play time!\n",
    "\n",
    "1. [Install splash](https://splash.readthedocs.io/en/stable/install.html#linux-docker) - The dockerized install is recommended, but I'm not the boss of you.\n",
    "2. Open http://127.0.0.1:8050 in your browser\n",
    "3. Capture a URL, look at what you get back\n",
    "\n",
    "\n",
    "**Problem**: Need quite a bit of lua knowledge, hard to compare subsequent captures, still now simple way to vizualize what is going on.\n",
    "\n",
    "Let's take a second and tink about a way to vizualize the problem we're trying to solve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
